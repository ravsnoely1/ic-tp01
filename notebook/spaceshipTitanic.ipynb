
# üõ† **Spaceship Titanic - An√°lise e Modelagem Completa**

## 1. Entendimento do Problema

O **Spaceship Titanic** √© uma competi√ß√£o do Kaggle que simula um cen√°rio fict√≠cio onde passageiros de uma nave espacial podem ter sido **transportados para outra dimens√£o** ap√≥s um acidente.  
O objetivo √© **prever se um passageiro foi transportado (`Transported = True/False`)** com base em informa√ß√µes como idade, gastos, cabine, destino, etc.

- **Tipo de problema:** Classifica√ß√£o bin√°ria.
- **M√©trica oficial:** **Accuracy**  
  F√≥rmula:
  \[
  Accuracy = \frac{\text{N√∫mero de previs√µes corretas}}{\text{Total de previs√µes}}
  \]
  Ou seja, a propor√ß√£o de acertos sobre o total de exemplos.

---

## 2. An√°lise Explorat√≥ria dos Dados (EDA)

```python
# =========================
# üì¶ Importa√ß√£o de bibliotecas
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler

# =========================
# üìÇ Carregamento dos dados
# =========================
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# Visualizar primeiras linhas
print(train.head())

# Estrutura do dataset
print(train.shape)
print(train.info())

# =========================
# üìâ Valores ausentes
# =========================
missing = train.isnull().sum().sort_values(ascending=False)
missing_percent = (missing / len(train)) * 100
missing_df = pd.DataFrame({"Missing": missing, "Percent": missing_percent})
print(missing_df)

# =========================
# üìä Distribui√ß√£o num√©rica
# =========================
num_cols = train.select_dtypes(include=np.number).columns
train[num_cols].hist(bins=30, figsize=(15, 8))
plt.show()

# =========================
# üìä Distribui√ß√£o categ√≥rica
# =========================
cat_cols = train.select_dtypes(exclude=np.number).columns
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(data=train, x=col)
    plt.xticks(rotation=45)
    plt.title(f"Distribui√ß√£o de {col}")
    plt.show()

# =========================
# üîç Correla√ß√£o
# =========================
plt.figure(figsize=(10,6))
sns.heatmap(train[num_cols].corr(), annot=True, cmap="coolwarm")
plt.title("Correla√ß√£o entre vari√°veis num√©ricas")
plt.show()

# =========================
# üéØ Rela√ß√£o com target
# =========================
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(data=train, x="Transported", y=col)
    plt.title(f"{col} vs Transported")
    plt.show()
```

**Decis√µes iniciais:**
- Vari√°veis como `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` t√™m valores ausentes que podem ser imputados com **mediana** (menos sens√≠vel a outliers).
- Vari√°veis categ√≥ricas como `HomePlanet`, `CryoSleep`, `Cabin`, `Destination` podem ser imputadas com **moda**.

---

## 3. Pr√©-Processamento

```python
# Copiar dataset para manipula√ß√£o
df = train.copy()

# Imputa√ß√£o de valores ausentes
for col in num_cols:
    df[col].fillna(df[col].median(), inplace=True)

for col in cat_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Codifica√ß√£o de vari√°veis categ√≥ricas
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Separar features e target
X = df.drop("Transported", axis=1)
y = df["Transported"].astype(int)

# Escalonamento
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split treino/teste
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
```

---

## 4. M√©trica de Avalia√ß√£o

```python
# Implementa√ß√£o manual
def accuracy_manual(y_true, y_pred):
    return np.mean(y_true == y_pred)

# Exemplo
y_true = np.array([1, 0, 1, 1])
y_pred = np.array([1, 0, 0, 1])
print("Accuracy manual:", accuracy_manual(y_true, y_pred))
```

---

## 5. Modelo Baseline

```python
from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(strategy="most_frequent")
cv = KFold(n_splits=5, shuffle=True, random_state=42)
baseline_acc = cross_val_score(dummy, X_scaled, y, cv=cv, scoring="accuracy")
print("Baseline Accuracy:", baseline_acc.mean())
```

---

## 6. Estudo de Solu√ß√µes Semelhantes

**Ideias comuns encontradas em notebooks Kaggle:**
- Engenharia de atributos: extrair n√∫mero da cabine, lado da nave.
- Imputa√ß√£o com KNN.
- Modelos ensemble (RandomForest, XGBoost, LightGBM).
- Feature selection com import√¢ncia de vari√°veis.

---

## 7. Modelos Diferentes

```python
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier

models = {
    "RandomForest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42, eval_metric="logloss"),
    "KNN": KNeighborsClassifier()
}

results = {}
for name, model in models.items():
    acc = cross_val_score(model, X_scaled, y, cv=cv, scoring="accuracy").mean()
    results[name] = acc

print(pd.DataFrame(results.items(), columns=["Modelo", "Accuracy"]))
```

---

## 8. Meta-Learning

```python
from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(
    estimators=[
        ("rf", RandomForestClassifier(random_state=42)),
        ("xgb", XGBClassifier(random_state=42, eval_metric="logloss")),
        ("knn", KNeighborsClassifier())
    ],
    voting="soft"
)

ensemble_acc = cross_val_score(voting, X_scaled, y, cv=cv, scoring="accuracy").mean()
print("VotingClassifier Accuracy:", ensemble_acc)
```

---

## 9. Otimiza√ß√£o de Hiperpar√¢metros com Optuna

```python
import optuna

def objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 100, 500)
    max_depth = trial.suggest_int("max_depth", 3, 15)
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42
    )
    return cross_val_score(rf, X_scaled, y, cv=cv, scoring="accuracy").mean()

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

print("Melhores par√¢metros:", study.best_params)
print("Melhor accuracy:", study.best_value)
```

---

## 10. Avalia√ß√£o Final

```python
final_results = pd.DataFrame([
    ["Baseline", baseline_acc.mean(), "-", "Modelo mais simples"],
    ["RandomForest", results["RandomForest"], "-", ""],
    ["XGBoost", results["XGBoost"], "-", ""],
    ["KNN", results["KNN"], "-", ""],
    ["VotingClassifier", ensemble_acc, "-", "Ensemble de 3 modelos"],
    ["RandomForest Tunado", study.best_value, study.best_params, "Optuna tuning"]
], columns=["Modelo", "Accuracy (CV)", "Melhor Par√¢metro", "Observa√ß√µes"])

print(final_results)
```

